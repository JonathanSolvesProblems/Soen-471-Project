{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fatal-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.functions import desc\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Preprocess(object):\n",
    "    def __init__(self, inputJsonDirectory, outputFileDirectory, outputJson):\n",
    "        self.inputJsonDirectory = inputJsonDirectory\n",
    "        self.outputFileDirectory = outputFileDirectory  \n",
    "        self.outputJson = outputJson\n",
    "        self.spark = self.init_spark()\n",
    "\n",
    "    def init_spark(self):\n",
    "        \n",
    "        spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Python Spark SQL basic example\") \\\n",
    "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "            .getOrCreate()\n",
    "        #conf = SparkConf().setAppName(\"test\").setMaster(\"local\")\n",
    "        #sc = SparkContext(conf=conf)\n",
    "        return spark\n",
    "\n",
    "    def getSpark(self):\n",
    "        return self.spark\n",
    "\n",
    "    def preprocessJson(self, inputJsonDirectory):\n",
    "        print(inputJsonDirectory)\n",
    "        df = self.spark.read.json(inputJsonDirectory)\n",
    "        print(df.count())\n",
    "\n",
    "        # df.show()\n",
    "        \n",
    "        df = self.flatten(df)\n",
    "\n",
    "        # Here we start dropping columns\n",
    "        \n",
    "        # remove -1 comments\n",
    "        df = df.filter(df.comments != -1)\n",
    "\n",
    "        # remove username\n",
    "        # df = df.drop('username')\n",
    "        drop_columns = ['username', 'bodywithurls', 'depth', 'depthRaw', 'lastseents', 'links', 'media', 'parent', 'posts', 'preview', 'state', 'urls_createdAt', 'urls_id', 'urls_modified',\n",
    "        'urls_short', 'urls_state']\n",
    "\n",
    "        df = df.drop(*drop_columns)\n",
    "\n",
    "        df.write.format(\"csv\").save(self.outputJson)\n",
    "        \n",
    "        # paralizing later, just for testing\n",
    "        # df.toPandas().to_csv(self.outputJson)\n",
    "\n",
    "        # df.printSchema()\n",
    "\n",
    "\n",
    "        # testing\n",
    "        # df.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"mydata.csv\")\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def flatten(self, df):\n",
    "        # compute Complex Fields (Lists and Structs) in Schema   \n",
    "        complex_fields = dict([(field.name, field.dataType)\n",
    "                                 for field in df.schema.fields\n",
    "                                 if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "        while len(complex_fields)!=0:\n",
    "          col_name=list(complex_fields.keys())[0]\n",
    "          print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "\n",
    "          # if StructType then convert all sub element to columns.\n",
    "          # i.e. flatten structs\n",
    "          if (type(complex_fields[col_name]) == StructType):\n",
    "             expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "             df=df.select(\"*\", *expanded).drop(col_name)\n",
    "\n",
    "          # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "          # i.e. explode Arrays\n",
    "          elif (type(complex_fields[col_name]) == ArrayType):    \n",
    "             df=df.withColumn(col_name,explode_outer(col_name))\n",
    "\n",
    "          # recompute remaining Complex Fields in Schema       \n",
    "          complex_fields = dict([(field.name, field.dataType)\n",
    "                                 for field in df.schema.fields\n",
    "                                 if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "        return df\n",
    "\n",
    "    def createResultDirectory(self):\n",
    "        # TODO: add outputFileDirectory, was getting weird error with it\n",
    "        # output_path = self.outputFileDirectory + self.outputJson\n",
    "        try:\n",
    "            f = open(self.outputJson, \"w\")\n",
    "            f.write(\"TODO: Add Results to CSV\")\n",
    "            f.close()\n",
    "        except:\n",
    "            sys.exit(\"Error: Unable to create file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consistent-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./parler_small.ndjson/\n",
      "19\n",
      "Processing :hashtags Type : <class 'pyspark.sql.types.ArrayType'>\n",
      "Processing :links Type : <class 'pyspark.sql.types.ArrayType'>\n",
      "Processing :urls Type : <class 'pyspark.sql.types.ArrayType'>\n",
      "Processing :urls Type : <class 'pyspark.sql.types.StructType'>\n",
      "Processing :urls_metadata Type : <class 'pyspark.sql.types.StructType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parlerDataDirectory = './parler_small.ndjson/'\n",
    "outputFileDirectory = './preprocessed/'\n",
    "outputJson = './parlers-data2/'\n",
    "\n",
    "# exception for testing, move to more appropriate place later.\n",
    "preprocessor = Preprocess(parlerDataDirectory, outputFileDirectory, outputJson)\n",
    "preprocessor.preprocessJson(parlerDataDirectory)\n",
    "# preprocessor.createResultDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-cowboy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
